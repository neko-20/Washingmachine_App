{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrape():\n",
    "    def __init__(self,wait=3,max=None):\n",
    "        self.response = None\n",
    "        self.df = pd.DataFrame()\n",
    "        self.wait = wait\n",
    "        self.max = max\n",
    "        self.timeout = 5\n",
    "\n",
    "    def request(self,url,encoding='utf-8',wait=None,max=None,console=True):\n",
    "        '''\n",
    "        指定したURLからページを取得する。\n",
    "        取得後にwaitで指定された秒数だけ待機する。\n",
    "        max が指定された場合、waitが最小値、maxが最大値の間でランダムに待機する。\n",
    "\n",
    "        Params\n",
    "        ---------------------\n",
    "        url:str\n",
    "            URL\n",
    "        encoding:str\n",
    "            ページのエンコード\n",
    "        wait:int\n",
    "            ウェイト秒\n",
    "        max:int\n",
    "            ウェイト秒の最大値\n",
    "        console:bool\n",
    "            状況をコンソール出力するか\n",
    "        Returns\n",
    "        ---------------------\n",
    "        soup:BeautifulSoupの戻り値\n",
    "        '''\n",
    "        self.wait = self.wait if wait is None else wait\n",
    "        self.max = self.max if max is None else max\n",
    "\n",
    "        start = time.time()\n",
    "        response = requests.get(url,timeout = self.timeout)\n",
    "        response.encoding = encoding\n",
    "        time.sleep(random.randint(self.wait,self.wait if self.max is None else self.max))\n",
    "\n",
    "        if console:\n",
    "            tm = datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S')\n",
    "            lap = time.time() - start\n",
    "            print(f'{tm} : {url}  経過時間 : {lap:.3f} 秒')\n",
    "\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    def get_href(self,soup,contains = None):\n",
    "        '''\n",
    "        soupの中からアンカータグを検索し、空でないurlをリストで返す\n",
    "        containsが指定された場合、更にその文字列が含まれるurlだけを返す\n",
    "\n",
    "        Params\n",
    "        ---------------------\n",
    "        soup:str\n",
    "            BeautifulSoupの戻り値\n",
    "        contains:str\n",
    "            抽出条件となる文字列\n",
    "\n",
    "        Returns\n",
    "        ---------------------\n",
    "        return :[str]\n",
    "            条件を満たすurlのリスト\n",
    "        '''\n",
    "        urls = list(set([url.get('href') for url in soup.find_all('a')]))\n",
    "        if contains is not None:\n",
    "           return [url for url in urls if self.contains(url,contains)]\n",
    "        return [url for url in urls if urls is not None or urls.strip() != '']\n",
    "\n",
    "    def get_src(self,soup,contains = None):\n",
    "        '''\n",
    "        soupの中からimgタグを検索し、空でないsrcをリストで返す\n",
    "        containsが指定された場合、更にその文字列が含まれるurlだけを返す\n",
    "\n",
    "        Params\n",
    "        ---------------------\n",
    "        soup:str\n",
    "            BeautifulSoupの戻り値\n",
    "        contains:str\n",
    "            抽出条件となる文字列\n",
    "\n",
    "        Returns\n",
    "        ---------------------\n",
    "        return :[str]\n",
    "            条件を満たすurlのリスト\n",
    "        '''\n",
    "        urls = list(set([url.get('src') for url in soup.find_all('img')]))\n",
    "        if contains is not None:\n",
    "           return [url for url in urls if contains(url,self.contains)]\n",
    "        return [url for url in urls if urls is not None or urls.strip() != '']\n",
    "\n",
    "    def contains(self,line,kwd):\n",
    "        '''\n",
    "        line に kwd が含まれているかチェックする。\n",
    "        line が None か '' の場合、或いは kwd が None 又は '' の場合は Trueを返す。\n",
    "\n",
    "        Params\n",
    "        ---------------------\n",
    "        line:str\n",
    "            HTMLの文字列\n",
    "        contains:str\n",
    "            抽出条件となる文字列\n",
    "\n",
    "        Returns\n",
    "        ---------------------\n",
    "        return :[str]\n",
    "            条件を満たすurlのリスト\n",
    "        '''\n",
    "        if line is None or line.strip() == '':\n",
    "            return False\n",
    "        if kwd is None or kwd == '':\n",
    "            return True\n",
    "        return kwd in line \n",
    "\n",
    "    def omit_char(self,values,omits):\n",
    "        '''\n",
    "        リストで指定した文字、又は文字列を削除する\n",
    "\n",
    "        Params\n",
    "        ---------------------\n",
    "        values:str\n",
    "            対象文字列\n",
    "        omits:str\n",
    "            削除したい文字、又は文字列\n",
    "\n",
    "        Returns\n",
    "        ---------------------\n",
    "        return :str\n",
    "            不要な文字を削除した文字列\n",
    "        '''\n",
    "        for n in range(len(values)):\n",
    "            for omit in omits:\n",
    "                values[n] = values[n].replace(omit,'')\n",
    "        return values\n",
    "\n",
    "    def add_df(self,values,columns,omits = None):\n",
    "        '''\n",
    "        指定した値を　DataFrame に行として追加する\n",
    "        omits に削除したい文字列をリストで指定可能\n",
    "\n",
    "        Params\n",
    "        ---------------------\n",
    "        values:[str]\n",
    "            列名\n",
    "        omits:[str]\n",
    "            削除したい文字、又は文字列\n",
    "        '''\n",
    "        if omits is not None:\n",
    "            values = self.omit_char(values,omits)\n",
    "            columns = self.omit_char(columns,omits)\n",
    "\n",
    "        df = pd.DataFrame(values,index=self.rename_column(columns))\n",
    "        self.df = pd.concat([self.df,df.T],ignore_index=True)\n",
    "\n",
    "    def to_csv(self,filename,dropcolumns=None):\n",
    "        '''\n",
    "        DataFrame をCSVとして出力する\n",
    "        dropcolumns に削除したい列をリストで指定可能\n",
    "\n",
    "        Params\n",
    "        ---------------------\n",
    "        filename:str\n",
    "            ファイル名\n",
    "        dropcolumns:[str]\n",
    "            削除したい列名\n",
    "        '''\n",
    "        if dropcolumns is not None:\n",
    "            self.df.drop(dropcolumns,axis=1,inplace=True)\n",
    "        self.df.insert(0,'id',self.df.index)\n",
    "        self.df.to_csv(filename,index=False)\n",
    "\n",
    "    def get_text(self,soup):\n",
    "        '''\n",
    "        渡された soup が Noneでなければ textプロパティの値を返す\n",
    "\n",
    "        Params\n",
    "        ---------------------\n",
    "        soup: bs4.element.Tag\n",
    "            bs4でfindした結果の戻り値\n",
    "\n",
    "        Returns\n",
    "        ---------------------\n",
    "        return :str\n",
    "            textプロパティに格納されている文字列\n",
    "        '''\n",
    "\n",
    "        return ' ' if soup == None else soup.text\n",
    "\n",
    "    def rename_column(self,columns):\n",
    "        '''\n",
    "        重複するカラム名の末尾に連番を付与し、ユニークなカラム名にする\n",
    "            例 ['A','B','B',B'] → ['A','B','B_1','B_2']\n",
    "\n",
    "        Params\n",
    "        ---------------------\n",
    "        columns: [str]\n",
    "            カラム名のリスト\n",
    "          \n",
    "        Returns\n",
    "        ---------------------\n",
    "        return :str\n",
    "            重複するカラム名の末尾に連番が付与されたリスト\n",
    "        '''\n",
    "        lst = list(set(columns))\n",
    "        for column in columns:\n",
    "            dupl = columns.count(column)\n",
    "            if dupl > 1:\n",
    "                cnt = 0\n",
    "                for n in range(0,len(columns)):\n",
    "                    if columns[n] == column:\n",
    "                        if cnt > 0:\n",
    "                            columns[n] = f'{column}_{cnt}'\n",
    "                        cnt += 1\n",
    "        return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_kakaku(url):\n",
    "    scr = Scrape(wait=3,max=5)\n",
    "\n",
    "    #レビューのURLからCategoryCD=2110/までを取り出す\n",
    "    url2 = url[:url.find('?lid')]\n",
    "\n",
    "    #製品画像のダウンロードディレクトリ作成\n",
    "    imgdir = \"./image\"\n",
    "    if not os.path.exists(imgdir):\n",
    "        os.mkdir(imgdir)\n",
    "\n",
    "    for n in range(1,316):\n",
    "        #リクエストするURLを設定\n",
    "        if n == 1:\n",
    "            target = url\n",
    "        else:\n",
    "            #商品の指定ページのURLを生成\n",
    "            target = url2+f'PageNo={n}/'\n",
    "\n",
    "        #レビューページの取得\n",
    "        soup = scr.request(target,encoding='shift_jis')\n",
    "        #ページ内の製品カテゴリを一括取得\n",
    "        categories = soup.find_all('table',class_='catebox')\n",
    "        #ページ内のレビュー記事を一括取得\n",
    "        reviews = soup.find_all('div',class_='revMainClmWrap')\n",
    "        #ページ内のすべてと評価を一括取得\n",
    "        evals = soup.find_all('div',class_='reviewBoxWtInner')\n",
    "\n",
    "        print(f'レビュー数:{len(reviews)}')\n",
    "\n",
    "        #ページ内の全てのレビューをループで取り出す\n",
    "        for category,review,eval in zip(categories,reviews,evals):\n",
    "            #製品画像を取得\n",
    "            imgurl = scr.get_src(category.find('td',class_='prdimg'))[0]\n",
    "            imgurl = imgurl.replace(\"/m/\", \"/l/\")    # 大きい画像にする\n",
    "            img = imgdir + \"/\" + os.path.basename(imgurl)\n",
    "            prdimg = \"Noimage.jpg\"    #NoImageの画像\n",
    "            r = requests.get(imgurl)\n",
    "            if r.status_code == 200:\n",
    "                with open(img, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                    prdimg = os.path.basename(imgurl)\n",
    "            time.sleep(1)\n",
    "            #製品名を取得\n",
    "            prdctgry = scr.get_text(category.find('td',class_='prdctgry')).split('>')\n",
    "            prdmaker = prdctgry[1].strip()\n",
    "            prdname = prdctgry[2 if (len(prdctgry) < 4) else 3].strip()\n",
    "            #レビューのタイトルを取得\n",
    "            title = scr.get_text(review.find('div',class_='reviewTitle'))\n",
    "            #レビューの内容を取得\n",
    "            comment = scr.get_text(review.find('p',class_='revEntryCont')).replace('<br>','')\n",
    "\n",
    "            #満足度（デザイン、使いやすさ、洗浄力、静音性、サイズ、機能・メニュー、・・・・・の値を取得\n",
    "            tables = eval.find_all('table')\n",
    "            star = scr.get_text(tables[0].find('td'))\n",
    "            date = scr.get_text(eval.find('p',class_='entryDate clearfix'))\n",
    "            date = date[:date.find('日')+1]\n",
    "            ths = tables[1].find_all('th')\n",
    "            tds = tables[1].find_all('td')\n",
    "\n",
    "            columns = ['prdimg','prdmaker','prdname','title','star','date','comment']\n",
    "            values = [prdimg,prdmaker,prdname,title,star,date,comment]\n",
    "\n",
    "            for th,td in zip(ths,tds):\n",
    "                columns.append(th.text)\n",
    "                values.append(td.text)\n",
    "\n",
    "            #レビューの詳細データを取得\n",
    "            detail = review.find('div',class_='revDetailData')\n",
    "            if detail is not None:\n",
    "                dl = detail.find('dl',class_='clearfix')\n",
    "                dts = dl.find_all('dt')\n",
    "                dds = dl.find_all('dd')\n",
    "                detail = ''\n",
    "                for dt, dd in zip(dts, dds):\n",
    "                    detail += ' ' + dt.text + ':' + dd.text\n",
    "                for dd in dds[len(dts):]:\n",
    "                    detail += '|' + dd.text\n",
    "                detail = detail.strip()\n",
    "            else:\n",
    "                detail = ''\n",
    "            columns.append('detail')\n",
    "            values.append(detail)\n",
    "\n",
    "            #DataFrameに登録\n",
    "            scr.add_df(values,columns,['<br>'])\n",
    "\n",
    "        #ページ内のレビュー数が15未満なら、最後のページと判断してループを抜ける\n",
    "        if len(reviews) < 15:\n",
    "            break\n",
    "\n",
    "    #スクレイプ結果をCSVに出力\n",
    "    scr.to_csv(\" \") #保存ディレクトリ指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_kakaku('') #URL指定"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
