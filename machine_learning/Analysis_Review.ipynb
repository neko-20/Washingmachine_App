{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, TfidfModel\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora\n",
    "from collections import defaultdict, Counter\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル読み込み\n",
    "model = KeyedVectors.load(' ') # word2vecモデル保存ディレクトリ指定\n",
    "\n",
    "# wordとvectorのリスト\n",
    "max_vocab = 50000\n",
    "vocab = list(model.wv.index_to_key)[:max_vocab]\n",
    "vectors = [model.wv[word] for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適なクラスタ数を調べる\n",
    "sse = []\n",
    "\n",
    "for c in range(2,15):                # 1~15クラスタまで一気に計算 \n",
    "    kmeans_model = KMeans(n_clusters=c, verbose=0, random_state=0)\n",
    "    kmeans_model.fit(vectors)\n",
    "    sse.append(kmeans_model.inertia_)\n",
    "\n",
    "plt.plot(range(2,15),sse,marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 洗濯機ワード作成\n",
    "def make_washing_word():\n",
    "\n",
    "    # モデル読み込み\n",
    "    model = KeyedVectors.load(' ') # word2vecモデル保存ディレクトリ指定\n",
    "\n",
    "    # wordとvectorのリスト\n",
    "    max_vocab = 30000\n",
    "    vocab = list(model.wv.index_to_key)[:max_vocab]\n",
    "    vectors = [model.wv[word] for word in vocab]\n",
    "\n",
    "    # k-meansクラスタリング\n",
    "    n_clusters = 6 #クラスタ数はこちらで任意の値を定める\n",
    "    kmeans_model = KMeans(n_clusters=n_clusters, n_init=10, random_state=0)\n",
    "    kmeans_model.fit(vectors)\n",
    "\n",
    "    # クラスタ辞書化\n",
    "    cluster_labels = kmeans_model.labels_\n",
    "    cluster_to_words = defaultdict(list)\n",
    "    for cluster_id, word in zip(cluster_labels, vocab):\n",
    "        cluster_to_words[cluster_id].append(word)\n",
    "\n",
    "    df_dict = pd.DataFrame.from_dict(cluster_to_words, orient=\"index\").T\n",
    "    print(df_dict.iloc[:20,:])\n",
    "\n",
    "    # 抽出したい分類のみwashing_wordに入れる\n",
    "    washing_word = []\n",
    "    \n",
    "\n",
    "    # out_top20.csv\n",
    "    for i in [0,1,2]:\n",
    "         washing_word.extend(cluster_to_words[i])\n",
    "\n",
    "    # 洗濯機ワードreturn\n",
    "    return washing_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 頻出トップ20作成\n",
    "def make_text_top20(review_words):\n",
    "    top20 = []\n",
    "    for words in review_words:\n",
    "        c = Counter(words)\n",
    "        c = c.most_common(20)\n",
    "        top20.append([i[0] for i in c])\n",
    "    return top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidfトップ20作成\n",
    "def make_tfidf_top20(review_words):\n",
    "    trainings = review_words[:]\n",
    "\n",
    "    # 単語->id変換の辞書作成\n",
    "    dictionary = corpora.Dictionary(trainings)\n",
    "\n",
    "    # textをcorpus化\n",
    "    corpus = list(map(dictionary.doc2bow, trainings))\n",
    "\n",
    "    # tfidf modelの生成\n",
    "    test_model = TfidfModel(corpus)\n",
    "\n",
    "    # corpusへのモデル適用\n",
    "    corpus_tfidf = test_model[corpus]\n",
    "\n",
    "    # id->単語へ変換\n",
    "    tfidf = [] # id -> 単語表示に変えた文書ごとのTF-IDF\n",
    "    for doc in corpus_tfidf:\n",
    "        words = []\n",
    "        for word in doc:\n",
    "            words.append([dictionary[word[0]], word[1]])\n",
    "        tfidf.append(words)\n",
    "\n",
    "    #TF-IDF値を高い順に並び替え上位単語20個に絞る。\n",
    "    top20 = [] \n",
    "    for l in tfidf:\n",
    "        l.sort(key=itemgetter(1), reverse=True)\n",
    "        l = l[:20]\n",
    "        top20.append([i[0] for i in l])\n",
    "\n",
    "    return top20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # 洗濯機ワード作成\n",
    "    washing_word = make_washing_word()\n",
    "\n",
    "    # レビューのワードをwashing_wordでフィルタリング\n",
    "    review_words = []\n",
    "\n",
    "    # out_top20.csv\n",
    "    with open(' ','r', encoding=\"utf-8\") as f: # コーパス保存ディレクトリ指定\n",
    "        for data in f:\n",
    "            word = data.replace(\"'\",'').replace('[','').replace(']','').replace(' ','').replace('\\n','').split(\",\")\n",
    "            review_words.append([i for i in word if i in washing_word])\n",
    "\n",
    "    # 頻出トップ20作成\n",
    "    text_top20 = make_text_top20(review_words)\n",
    "\n",
    "    # tfidfトップ20作成\n",
    "    tfidf_top20 = make_tfidf_top20(review_words)\n",
    "\n",
    "    # 結果をデータフレームにしてcsvに書き出す\n",
    "    df = pd.read_csv(' ') # レビュー格納csvを指定\n",
    "    df_washing = df.groupby(['prdname','prdmaker','prdimg'])['star'].mean(numeric_only=True).reset_index().sort_values('star', ascending=False)\n",
    "    df_washing['star'] = df_washing['star'].round(1)\n",
    "    df_washing = df_washing.sort_values('prdname')\n",
    "    df_washing['text_top20'] = text_top20\n",
    "    df_washing['tfidf_top20'] = tfidf_top20\n",
    "    df_washing = df_washing.sort_values('star', ascending=False)\n",
    "    df_washing['id'] = ['ID-' + str(i + 1).zfill(3) for i in range(len(df_washing.index))]\n",
    "    df_washing_sorted_top20 = df_washing.loc[:,['id','prdname','prdmaker','prdimg','star','text_top20','tfidf_top20']].reset_index(drop=True)\n",
    "    df_washing_sorted_top20.to_csv(\"out_top20.csv\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
