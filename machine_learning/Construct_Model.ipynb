{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim.models import word2vec\n",
    "import MeCab\n",
    "import neologdn\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('') #NEologd辞書指定\n",
    "\n",
    "tagger.parse('')\n",
    "def tokenize_ja(text, lower):\n",
    "    node = tagger.parseToNode(str(text))\n",
    "    while node:\n",
    "        if lower and node.feature.split(',')[0] in [\"名詞\",\"形容詞\"]:#分かち書きで取得する品詞を指定\n",
    "#         if lower and node.feature.split(',')[0] in [\"名詞\",\"形容詞\",\"副詞\"]:#model5\n",
    "            yield node.surface.lower()\n",
    "        node = node.next\n",
    "def tokenize(content, token_min_len, token_max_len, lower):\n",
    "    return [\n",
    "        str(token) for token in tokenize_ja(content, lower)\n",
    "        if token_min_len <= len(token) <= token_max_len and not token.startswith('_')\n",
    "    ]\n",
    "\n",
    "    \n",
    "#学習データの読み込み\n",
    "df_machie = pd.read_csv('') # レビュー格納csvを指定\n",
    "df_wash = df_machie.groupby(['prdname'])['comment'].apply(list).apply(' '.join).reset_index().sort_values('prdname')\n",
    "\n",
    "def remove_unnecessary(text):\n",
    "    # 半角と全角の統一と重ね表現の除去\n",
    "    normalized_text = neologdn.normalize(text)\n",
    "    #URLの除去\n",
    "    text_without_url = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', normalized_text)\n",
    "    #桁区切りの除去\n",
    "    tmp = re.sub(r'(\\d)([,.])(\\d+)', r'\\1\\3', text_without_url)\n",
    "    # 半角記号の置換\n",
    "    tmp = re.sub(r'[!-/:-@[-`{-~]', r' ', tmp)\n",
    "    # 全角記号の置換 (ここでは0x25A0 - 0x266Fのブロックのみを除去)\n",
    "    text_removed_symbol = re.sub(u'[■-♯]', ' ', tmp)\n",
    "    \n",
    "    return(text_removed_symbol)\n",
    "\n",
    "\n",
    "#コーパス作成\n",
    "wakati_washing_text = []\n",
    "for i in df_wash['comment']:\n",
    "    text = remove_unnecessary(i)\n",
    "    wakati = tokenize(text, 2, 10000, True)\n",
    "    wakati_washing_text.append(wakati)\n",
    "\n",
    "wakati_washing_list = np.asarray(wakati_washing_text, dtype = object) \n",
    "np.savetxt(\" \", wakati_washing_list, fmt = '%s', delimiter = ',', encoding = 'utf-8') #コーパス保存ディレクトリ指定\n",
    "# np.savetxt(\"washing_corpus5.txt\", wakati_washing_list, fmt = '%s', delimiter = ',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル作成\n",
    "word2vec_washing_model = word2vec.Word2Vec(wakati_washing_text, sg = 1, vector_size = 100, window = 5, min_count = 3, epochs = 100, workers = 3)\n",
    "# モデルのセーブ\n",
    "word2vec_washing_model.save(\" \") #word2vecモデル保存ディレクトリ指定"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
